{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install dependencies\n",
    "!pip3 install tqdm      #shows a smart progress meter while executing the loops in code\n",
    "!pip3 install pillow --upgrade      #for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile    #for reading and writing tar archive files\n",
    "import urllib     #for handling urls\n",
    "import shutil     #for high level operations on files and collection of files\n",
    "import json       #javascript object notation\n",
    "import random\n",
    "import boto3      #to create, configure and maange aws services such as EC2 and S3\n",
    "import sagemaker  #for training and deploying machine learning models on amazon sagemaker\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import matplotlib.pyplot as plt\n",
    "from xml.etree import ElementTree as ET     #to read and manipulate xml tree structures\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download and Extract the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url for the oxford IIIT pets dataset\n",
    "urls = [\"http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\",\n",
    "       \"http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to download and extract the data\n",
    "\n",
    "def download_and_extract(data_dir, download_dir):\n",
    "    \n",
    "    \"\"\"Function to download and extract the Oxford IIIT Pets Dataset.\n",
    "    Input args: data_dir = directory where the data will be downloaded\n",
    "                download_dir = url for the data to be downloaded\"\"\"\n",
    "    \n",
    "    for url in urls:\n",
    "        target_file = url.split(\"/\")[-1]    #access the file name from the url\n",
    "        if target_file not in os.listdir(download_dir):\n",
    "            print(\"Downloading\", url)\n",
    "            urllib.request.urlretrieve(url, os.path.join(download_dir, target_file))  #retrieve the url in a temporary location in disk\n",
    "            tf = tarfile.open(url.split(\"/\")[-1])\n",
    "            tf.extractall(data_dir)\n",
    "        else:\n",
    "            print(\"Data already downloaded\", url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data directory if it does not exist\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and extract the data\n",
    "download_and_extract(\"data\", \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the annotations from the XML Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now that the data is downloaded we have acquired the images for training our model. But the annotations are in XML format. If we look into one of the XML files we will see it contains the object class information and also the bound boxes which indicates the size of the face of the pet object that we will train our model on. So, the next step becomes extracting these information from the xml files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_dir = \"data/annotations/xmls\"    #define the xml directory\n",
    "xml_files = [os.path.join(xml_dir, x) for x in os.listdir(xml_dir) if x[-3:] == \"xml\"]  #make a list of paths of xml files\n",
    "xml_files[0]   #print the first element of the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(xml_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classes of cat and dog and give them corresponding class id\n",
    "classes = [\"cat\", \"dog\"]\n",
    "categories = [\n",
    "    {\n",
    "        \"class_id\":0,\n",
    "        \"name\":\"cat\"\n",
    "    },\n",
    "    {\n",
    "        \"class_id\":1,\n",
    "        \"name\":\"dog\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In the next step we extract the files from xml_files and convert them in a json format. This is because when we use mxnet in aws sagemaker it expects the data to be structured in a json file format. So, now we convert the xml annotations to json annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_annotations_xml_to_json(xml_file_path):\n",
    "    \n",
    "    \"\"\"Function to convert the xml annotations into json annotations\"\"\"\n",
    "    \n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    annotation = {}\n",
    "    \n",
    "    annotation[\"file\"] = root.find(\"filename\").text\n",
    "    annotation[\"categories\"] = categories\n",
    "    size = root.find(\"size\")\n",
    "    annotation[\"image_size\"] = [{\n",
    "        \"width\": int(size.find(\"width\").text),\n",
    "        \"height\": int(size.find(\"height\").text),\n",
    "        \"depth\": int(size.find(\"depth\").text)\n",
    "    }]\n",
    "    \n",
    "    annotation[\"annotations\"] = []\n",
    "    \n",
    "    for item in root.iter(\"object\"):\n",
    "        class_id = classes.index(item.find(\"name\").text)\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "        \n",
    "        for box in item.findall(\"bndbox\"):\n",
    "            xmin = int(box.find(\"xmin\").text)\n",
    "            ymin = int(box.find(\"ymin\").text)\n",
    "            xmax = int(box.find(\"xmax\").text)\n",
    "            ymax = int(box.find(\"ymax\").text)\n",
    "            \n",
    "            if all([xmin, ymin, xmax, ymax]) is not None:\n",
    "                annotation[\"annotations\"].append({\n",
    "                    \"class_id\":class_id,\n",
    "                    \"left\":xmin,\n",
    "                    \"top\":ymin,\n",
    "                    \"width\":xmax - xmin,\n",
    "                    \"height\":ymax - ymin\n",
    "                })\n",
    "                \n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_annotations_xml_to_json(xml_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(plt, annot, image_file_path, rows, cols, index):\n",
    "    \n",
    "    \"\"\"Function to plot image from the data\"\"\"\n",
    "    \n",
    "    img = Image.open(image_file_path)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/dejavu/DejaVuSerif-Bold.ttf\", 20)\n",
    "    \n",
    "    for a in annot[\"annotations\"]:\n",
    "        box = [\n",
    "            int(a[\"left\"]), int(a[\"top\"]),\n",
    "            int(a[\"left\"]) + int(a[\"width\"]),\n",
    "            int(a[\"top\"]) + int(a[\"height\"])\n",
    "        ]\n",
    "        draw.rectangle(box, outline = \"yellow\", width = 4)\n",
    "        draw.text((box[0], box[1]), classes[int(a[\"class_id\"])], font = font)\n",
    "    plt.subplot(rows, cols, index + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_annotations(plt):\n",
    "    \n",
    "    \"\"\"Function to show random annotations from the data\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    for i in range(0,9):\n",
    "        index = random.randint(0, len(xml_files)-1)\n",
    "        annot = convert_annotations_xml_to_json(xml_files[index])\n",
    "        image_file_path = os.path.join(\"data/images/\", annot[\"file\"])\n",
    "        \n",
    "        plt = plot_image(plt, annot, image_file_path, 3, 3, i)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We see from the random image plots that all the faces are annotated. The bound box allows us to annotate only the faces. We also see that the boxes also contain the class of the image i.e., cat and dog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()     # set up an execution role\n",
    "bucket = \"intekhabpetsdata\"    # bucket to store all the training data\n",
    "\n",
    "# training image is a docker image where all our code will be executed during training\n",
    "training_image = get_image_uri(boto3.Session().region_name, \"object-detection\", repo_version = \"latest\")\n",
    "print(training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create blank folders according to the sagemaker object-detection algorithm. the structure of the folders is unique\n",
    "# and must be the same for every case\n",
    "\n",
    "folders = [\"train\", \"train_annotation\", \"validation\", \"validation_annotation\"]\n",
    "\n",
    "for folder in folders:\n",
    "    if os.path.isdir(folder):\n",
    "        shutil.rmtree(folder)\n",
    "    os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After doing this we need to create the bucket in S3 so that we can store all our data there. At this stage we go to AWS console and create a bucket with the same name that we already created in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preparation of Data for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the length of our dataset\n",
    "print(\"Total number of samples:\", len(xml_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml_file in tqdm(xml_files):\n",
    "    target_set = \"train\" if random.randint(0,99) < 75 else \"validation\" # randomly select samples for train and validation\n",
    "    annot = convert_annotations_xml_to_json(xml_file) # extract the annotations for each file\n",
    "    image_file_path = os.path.join(\"data/images/\", annot[\"file\"])\n",
    "    image_target_path = os.path.join(target_set, annot[\"file\"])\n",
    "    shutil.copy(image_file_path, image_target_path)\n",
    "    json_file_path = os.path.join(target_set + \"_annotation\", annot[\"file\"][:-3] + \"json\")\n",
    "    with open(json_file_path, \"w\") as f:\n",
    "        json.dump(annot, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = os.listdir(\"train\")\n",
    "train_annots = os.listdir(\"train_annotation\")\n",
    "print(len(train_images), len(train_annots))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the image name in train and annotation folder correspondingly match with each other\n",
    "for image in train_images:\n",
    "    key = image.split(\".\")[0]\n",
    "    json_file = key + \".json\"\n",
    "    if json_file not in train_annots:\n",
    "        print(\"Not found\", json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upload the Data to S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We upload the data to S3 now so that when we start our training session with SageMaker, SageMaker will create a virtual machine. All our computations will be carried out inside this virtual machine and the data required for the training process will be directly downloaded from the S3 bucket to the virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()    #create a sagemaker session to upload the data\n",
    "\n",
    "print(\"Uploading data...\")\n",
    "s3_train_path = sess.upload_data(path = \"train\", bucket = bucket, key_prefix = \"train\")\n",
    "print(\"Training images uploaded!\")\n",
    "s3_validation_path = sess.upload_data(path = \"validation\", bucket = bucket, key_prefix = \"validation\")\n",
    "print(\"Validation images uploaded!\")\n",
    "s3_train_annot_path = sess.upload_data(path = \"train_annotation\", bucket = bucket, key_prefix = \"train_annotation\")\n",
    "print(\"Train annontations uploaded!\")\n",
    "s3_validation_annot_path = sess.upload_data(path = \"validation_annotation\", bucket = bucket,\n",
    "                                            key_prefix = \"validation_annotation\")\n",
    "print(\"Validation annotations uploaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_validation_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_annot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_validation_annot_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SageMaker Estimator\n",
    "SageMaker Estimator is a high level API which will handle the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role = role,\n",
    "    train_instance_type = \"ml.p3.2xlarge\",   #16 GB GPU\n",
    "    train_instance_count = 1,\n",
    "    train_volume_size = 100,                 #100 GB\n",
    "    train_max_run = 36000,                   # After 36000 seconds force stop\n",
    "    input_mode = \"File\",                     # When using json annotations we use file input mode\n",
    "    output_path = \"s3://intekhabpetsdata/output\",    # Store the data in a new folder called output in the bucket\n",
    "    sagemaker_session = sess\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set up the Hyperparameters\n",
    "These hyperparameters are specific to the object-detection algorithm in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_hyperparameters(\n",
    "    base_network = \"resnet-50\",    # for objet detection we can use resnet-50 or vgg\n",
    "    num_classes = 2,               # cat and dog\n",
    "    use_pretrained_model = 1,      # 1 means we will load the pretrained weights\n",
    "    mini_batch_size = 16,\n",
    "    epochs = 15,\n",
    "    learning_rate = 0.001,\n",
    "    optimizer = \"sgd\",             # stochastic gradient descent\n",
    "    lr_scheduler_step = \"10\",\n",
    "    lr_scheduler_factor = 0.1,\n",
    "    momentum = 0.9,\n",
    "    weight_decay = 0.0005,\n",
    "    overlap_threshold = 0.5,\n",
    "    nms_threshold = 0.45,\n",
    "    image_shape = 512,\n",
    "    num_training_samples = len(train_annots)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the Data Channels\n",
    "We need to specify the paths to access the data for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_path, distribution = \"FullyReplicated\",\n",
    "                                       content_type = \"application/x-image\", s3_data_type = \"S3Prefix\")\n",
    "validation_data = sagemaker.session.s3_input(s3_validation_path, distribution = \"FullyReplicated\",\n",
    "                                       content_type = \"application/x-image\", s3_data_type = \"S3Prefix\")\n",
    "train_annotation_data = sagemaker.session.s3_input(s3_train_annot_path, distribution = \"FullyReplicated\",\n",
    "                                       content_type = \"application/x-image\", s3_data_type = \"S3Prefix\")\n",
    "validation_annotation_data = sagemaker.session.s3_input(s3_validation_annot_path, distribution = \"FullyReplicated\",\n",
    "                                       content_type = \"application/x-image\", s3_data_type = \"S3Prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    \"train\":train_data,\n",
    "    \"validation\":validation_data,\n",
    "    \"train_annotation\":train_annotation_data,\n",
    "    \"validation_annotation\":validation_annotation_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
